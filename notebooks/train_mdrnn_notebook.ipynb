{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Recurrent model training (memory)\"\"\"\n",
    "import argparse\n",
    "from functools import partial\n",
    "from os.path import join, exists\n",
    "from os import mkdir\n",
    "import torch\n",
    "import torch.nn.functional as f\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from utils.misc import save_checkpoint\n",
    "from utils.misc import ASIZE, LSIZE, RSIZE, RED_SIZE, SIZE\n",
    "from utils.learning import EarlyStopping\n",
    "## WARNING : THIS SHOULD BE REPLACED WITH PYTORCH 0.5\n",
    "from utils.learning import ReduceLROnPlateau\n",
    "\n",
    "from data.loaders import RolloutSequenceDataset\n",
    "from models.vae import VAE\n",
    "from models.mdrnn import MDRNN, gmm_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\"MDRNN training\")\n",
    "parser.add_argument('--logdir', type=str,\n",
    "                    help=\"Where things are logged and models are loaded from.\")\n",
    "parser.add_argument('--epochs', type=int, default=60, metavar='N',\n",
    "                    help='number of epochs to train (default: 1000)')\n",
    "parser.add_argument('--noreload', action='store_true',\n",
    "                    help=\"Do not reload if specified.\")\n",
    "parser.add_argument('--include_reward', action='store_true',\n",
    "                    help=\"Add a reward modelisation term to the loss.\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vae():\n",
    "    # Loading VAE\n",
    "    vae_file = join(args.logdir, 'vae', 'best.tar')\n",
    "    assert exists(vae_file), \"No trained VAE in the logdir...\"\n",
    "    state = torch.load(vae_file)\n",
    "    print(\"Loading VAE at epoch {} \"\n",
    "        \"with test error {}\".format(\n",
    "            state['epoch'], state['precision']))\n",
    "\n",
    "    vae = VAE(ASIZE+1, LSIZE).to(device)\n",
    "    vae.load_state_dict(state['state_dict'])\n",
    "    return vae, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    dataset_train = RolloutSequenceDataset('datasets/loadbalancing', SEQ_LEN, train=True, buffer_size=30)\n",
    "    dataset_test = RolloutSequenceDataset('datasets/loadbalancing', SEQ_LEN, train=False, buffer_size=10)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset_train, batch_size=BSIZE, shuffle=True, num_workers=8)\n",
    "    test_loader = DataLoader(\n",
    "        dataset_test, batch_size=BSIZE, num_workers=8)\n",
    "\n",
    "    return dataset_test, dataset_train, test_loader, train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_action(action):\n",
    "    # Create a zero tensor for one-hot encoding\n",
    "    one_hot_actions = torch.zeros(SEQ_LEN, BSIZE, ASIZE)\n",
    "    # Fill with ones at the corresponding indices\n",
    "    for i in range(action.size(0)):\n",
    "        for j in range(action.size(1)):\n",
    "            action_index = action[i, j].long()  # convert to long for indexing\n",
    "            one_hot_actions[i, j, action_index] = 1\n",
    "    return one_hot_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_latent(obs, next_obs):\n",
    "    \"\"\" Transform observations to latent space.\n",
    "\n",
    "    :args obs: 5D torch tensor (BSIZE, SEQ_LEN, ASIZE, SIZE, SIZE)\n",
    "    :args next_obs: 5D torch tensor (BSIZE, SEQ_LEN, ASIZE, SIZE, SIZE)\n",
    "\n",
    "    :returns: (latent_obs, latent_next_obs)\n",
    "        - latent_obs: 4D torch tensor (BSIZE, SEQ_LEN, LSIZE)\n",
    "        - next_latent_obs: 4D torch tensor (BSIZE, SEQ_LEN, LSIZE)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # print(\"Original obs shape:\", obs.shape)\n",
    "        # print(\"Original next_obs shape:\", next_obs.shape)\n",
    "\n",
    "        # Original input processed by VAE - VAE forward method returns (reconstructed_x, mean_latent_space, logsigma_latent_space)\n",
    "        obs_results, next_obs_results = vae(obs)[1:], vae(next_obs)[1:]\n",
    "\n",
    "        # print(\"VAE processed obs shape:\", obs_results[0].shape)\n",
    "        # print(\"VAE processed next_obs shape:\", next_obs_results[0].shape)\n",
    "\n",
    "        # Extract mu and logsigma for both obs and next_obs\n",
    "        obs_mu, obs_logsigma = obs_results\n",
    "        next_obs_mu, next_obs_logsigma = next_obs_results\n",
    "\n",
    "        # # Exclude the last element of obs and the first element of next_obs\n",
    "        # obs_mu, obs_logsigma = obs_mu[:, :-1, :], obs_logsigma[:, :-1, :]\n",
    "        # next_obs_mu, next_obs_logsigma = next_obs_mu[:, 1:, :], next_obs_logsigma[:, 1:, :]\n",
    "\n",
    "        # Process the results for obs and next_obs\n",
    "        latent_obs, latent_next_obs = [\n",
    "            (x_mu + x_logsigma.exp() * torch.randn_like(x_mu))\n",
    "            for x_mu, x_logsigma in\n",
    "            [(obs_mu, obs_logsigma), (next_obs_mu, next_obs_logsigma)]]\n",
    "        \n",
    "        # print(\"latent_obs shape:\", latent_obs.shape)\n",
    "        # print(\"latent_next_obs shape:\", latent_next_obs.shape)\n",
    "    \n",
    "        return latent_obs, latent_next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(latent_obs, action, reward, terminal,\n",
    "             latent_next_obs, include_reward: bool):\n",
    "    \"\"\" Compute losses.\n",
    "\n",
    "    The loss that is computed is:\n",
    "    (GMMLoss(latent_next_obs, GMMPredicted) + MSE(reward, predicted_reward) +\n",
    "         BCE(terminal, logit_terminal)) / (LSIZE + 2)\n",
    "    The LSIZE + 2 factor is here to counteract the fact that the GMMLoss scales\n",
    "    approximately linearily with LSIZE. All losses are averaged both on the\n",
    "    batch and the sequence dimensions (the two first dimensions).\n",
    "\n",
    "    :args latent_obs: (BSIZE, SEQ_LEN, LSIZE) torch tensor\n",
    "    :args action: (BSIZE, SEQ_LEN, ASIZE) torch tensor\n",
    "    :args reward: (BSIZE, SEQ_LEN) torch tensor\n",
    "    :args latent_next_obs: (BSIZE, SEQ_LEN, LSIZE) torch tensor\n",
    "\n",
    "    :returns: dictionary of losses, containing the gmm, the mse, the bce and\n",
    "        the averaged loss.\n",
    "    \"\"\"\n",
    "    latent_obs, action,\\\n",
    "        reward, terminal,\\\n",
    "        latent_next_obs = [arr.transpose(1, 0)\n",
    "                           for arr in [latent_obs, action,\n",
    "                                       reward, terminal,\n",
    "                                       latent_next_obs]]\n",
    "\n",
    "    action = process_action(action)\n",
    "    # print(\"action shape: \", action.shape)\n",
    "    mus, sigmas, logpi, rs, ds = mdrnn(action, latent_obs)\n",
    "    gmm = gmm_loss(latent_next_obs, mus, sigmas, logpi)\n",
    "    bce = f.binary_cross_entropy_with_logits(ds, terminal)\n",
    "    if include_reward:\n",
    "        mse = f.mse_loss(rs, reward)\n",
    "        scale = LSIZE + 2\n",
    "    else:\n",
    "        mse = 0\n",
    "        scale = LSIZE + 1\n",
    "    loss = (gmm + bce + mse) / scale\n",
    "    return dict(gmm=gmm, bce=bce, mse=mse, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pass(epoch, train, include_reward): # pylint: disable=too-many-locals\n",
    "    \"\"\" One pass through the data \"\"\"\n",
    "    if train:\n",
    "        mdrnn.train()\n",
    "        loader = train_loader\n",
    "    else:\n",
    "        mdrnn.eval()\n",
    "        loader = test_loader\n",
    "\n",
    "    loader.dataset.load_next_buffer()\n",
    "\n",
    "    cum_loss = 0\n",
    "    cum_gmm = 0\n",
    "    cum_bce = 0\n",
    "    cum_mse = 0\n",
    "\n",
    "    pbar = tqdm(total=len(loader.dataset), desc=\"Epoch {}\".format(epoch))\n",
    "    for i, data in enumerate(loader):\n",
    "        obs, action, reward, terminal, next_obs = [arr.float().to(device) for arr in data]\n",
    "        # print(\"\\n--------------------\")\n",
    "        # print(\"action shape: \", action.shape)\n",
    "        # print(\"latent_obs shape: \", obs.shape)\n",
    "        # print(\"reward shape: \", reward.shape)\n",
    "        # print(\"terminal shape: \", terminal.shape)\n",
    "        # print(\"--------------------\\n\")\n",
    "\n",
    "        # transform obs\n",
    "        latent_obs, latent_next_obs = to_latent(obs, next_obs)\n",
    "\n",
    "        if train:\n",
    "            losses = get_loss(latent_obs, action, reward,\n",
    "                              terminal, latent_next_obs, include_reward)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses['loss'].backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                losses = get_loss(latent_obs, action, reward,\n",
    "                                  terminal, latent_next_obs, include_reward)\n",
    "\n",
    "        cum_loss += losses['loss'].item()\n",
    "        cum_gmm += losses['gmm'].item()\n",
    "        cum_bce += losses['bce'].item()\n",
    "        cum_mse += losses['mse'].item() if hasattr(losses['mse'], 'item') else \\\n",
    "            losses['mse']\n",
    "\n",
    "        pbar.set_postfix_str(\"loss={loss:10.6f} bce={bce:10.6f} \"\n",
    "                             \"gmm={gmm:10.6f} mse={mse:10.6f}\".format(\n",
    "                                 loss=cum_loss / (i + 1), bce=cum_bce / (i + 1),\n",
    "                                 gmm=cum_gmm / LSIZE / (i + 1), mse=cum_mse / (i + 1)))\n",
    "        pbar.update(BSIZE)\n",
    "    pbar.close()\n",
    "    return cum_loss * BSIZE / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # constants\n",
    "    BSIZE = 16\n",
    "    SEQ_LEN = 32\n",
    "    # epochs = 30\n",
    "\n",
    "    # Loading VAE\n",
    "    vae, state = load_vae()\n",
    "\n",
    "    # Loading model (if it exists already)\n",
    "    rnn_dir = join(args.logdir, 'mdrnn')\n",
    "    rnn_file = join(rnn_dir, 'best.tar')\n",
    "\n",
    "    if not exists(rnn_dir):\n",
    "        mkdir(rnn_dir)\n",
    "\n",
    "    mdrnn = MDRNN(LSIZE, ASIZE, RSIZE, 5)\n",
    "    mdrnn.to(device)\n",
    "    optimizer = torch.optim.RMSprop(mdrnn.parameters(), lr=1e-3, alpha=.9)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5)\n",
    "    earlystopping = EarlyStopping('min', patience=30)\n",
    "\n",
    "    if exists(rnn_file) and not args.noreload:\n",
    "        rnn_state = torch.load(rnn_file)\n",
    "        print(\"Loading MDRNN at epoch {} \"\n",
    "            \"with test error {}\".format(\n",
    "                rnn_state[\"epoch\"], rnn_state[\"precision\"]))\n",
    "        mdrnn.load_state_dict(rnn_state[\"state_dict\"])\n",
    "        optimizer.load_state_dict(rnn_state[\"optimizer\"])\n",
    "        scheduler.load_state_dict(state['scheduler'])\n",
    "        earlystopping.load_state_dict(state['earlystopping'])\n",
    "\n",
    "\n",
    "    # Data Loading\n",
    "    dataset_test, dataset_train, test_loader, train_loader = load_data()\n",
    "    # partial() is used to create a new function train() and test() from data_pass(), one with \n",
    "    # train=True and the other with train=False\n",
    "    train = partial(data_pass, train=True, include_reward=args.include_reward)\n",
    "    test = partial(data_pass, train=False, include_reward=args.include_reward)\n",
    "\n",
    "    cur_best = None\n",
    "    for e in range(args.epochs):\n",
    "        train(e)\n",
    "        test_loss = test(e)\n",
    "        scheduler.step(test_loss)\n",
    "        earlystopping.step(test_loss)\n",
    "\n",
    "        is_best = not cur_best or test_loss < cur_best\n",
    "        if is_best:\n",
    "            cur_best = test_loss\n",
    "        checkpoint_fname = join(rnn_dir, 'checkpoint.tar')\n",
    "        save_checkpoint({\n",
    "            \"state_dict\": mdrnn.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'earlystopping': earlystopping.state_dict(),\n",
    "            \"precision\": test_loss,\n",
    "            \"epoch\": e}, is_best, checkpoint_fname,\n",
    "                        rnn_file)\n",
    "\n",
    "        if earlystopping.stop:\n",
    "            print(\"End of Training because of early stopping at epoch {}\".format(e))\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
