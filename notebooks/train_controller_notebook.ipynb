{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cma in /Users/pedrosousa/anaconda3/lib/python3.11/site-packages (3.3.0)\n",
      "Requirement already satisfied: numpy in /Users/pedrosousa/anaconda3/lib/python3.11/site-packages (from cma) (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install cma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on Google Colab. Assuming local environment.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check if the notebook is running on Colab\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    # This block will run only in Google Colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running on Google Colab. Cloning the repository.\")\n",
    "    !git clone https://github.com/pedro15sousa/R244-wm-load-balancing.git\n",
    "    %cd R244-wm-load-balancing/notebooks\n",
    "else: \n",
    "    # This block will run if not in Google Colab\n",
    "    IN_COLAB = False\n",
    "    print(\"Not running on Google Colab. Assuming local environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')  # This adds the parent directory (main_folder) to the Python path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training a linear controller on latent + recurrent state\n",
    "with CMAES.\n",
    "\n",
    "This is a bit complex. num_workers slave threads are launched\n",
    "to process a queue filled with parameters to be evaluated.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import sys\n",
    "from os.path import join, exists\n",
    "from os import mkdir, unlink, listdir, getpid\n",
    "from time import sleep\n",
    "from torch.multiprocessing import Process, Queue\n",
    "import torch\n",
    "import cma\n",
    "from models import Controller\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from utils.misc import RolloutGenerator, ASIZE, RSIZE, LSIZE\n",
    "from utils.misc import load_parameters\n",
    "from utils.misc import flatten_parameters\n",
    "from utils.misc import slave_routine, set_slave_routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--logdir', type=str, help='Where everything is stored.')\n",
    "# parser.add_argument('--n-samples', type=int, help='Number of samples used to obtain '\n",
    "#                     'return estimate.')\n",
    "# parser.add_argument('--pop-size', type=int, help='Population size.')\n",
    "# parser.add_argument('--target-return', type=float, help='Stops once the return '\n",
    "#                     'gets above target_return')\n",
    "# parser.add_argument('--display', action='store_true', help=\"Use progress bars if \"\n",
    "#                     \"specified.\")\n",
    "# parser.add_argument('--max-workers', type=int, help='Maximum number of workers.',\n",
    "#                     default=32)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "args = {\n",
    "    'n_samples': 4,   # input batch size for training (default: 32)\n",
    "    'epochs': 60,     # number of epochs to train (default: 1000)\n",
    "    'logdir': '../exp_dir',  # Directory where results are logged\n",
    "    'pop_size': 4,\n",
    "    'target_return': -2000000,\n",
    "    \"display\": True,\n",
    "    \"max_workers\": 2, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max number of workers. M\n",
    "\n",
    "# multiprocessing variables\n",
    "n_samples = args['n_samples']\n",
    "pop_size = args[\"pop_size\"]\n",
    "num_workers = min(args[\"max_workers\"], n_samples * pop_size)\n",
    "time_limit = 10000\n",
    "\n",
    "# create tmp dir if non existent and clean it if existent\n",
    "tmp_dir = join(args['logdir'], 'tmp')\n",
    "if not exists(tmp_dir):\n",
    "    mkdir(tmp_dir)\n",
    "else:\n",
    "    for fname in listdir(tmp_dir):\n",
    "        unlink(join(tmp_dir, fname))\n",
    "\n",
    "# create ctrl dir if non exitent\n",
    "ctrl_dir = join(args['logdir'], 'ctrl')\n",
    "if not exists(ctrl_dir):\n",
    "    mkdir(ctrl_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                           Thread routines                                    #\n",
    "################################################################################\n",
    "def slave_routine(p_queue, r_queue, e_queue, p_index):\n",
    "    \"\"\" Thread routine.\n",
    "\n",
    "    Threads interact with p_queue, the parameters queue, r_queue, the result\n",
    "    queue and e_queue the end queue. They pull parameters from p_queue, execute\n",
    "    the corresponding rollout, then place the result in r_queue.\n",
    "\n",
    "    Each parameter has its own unique id. Parameters are pulled as tuples\n",
    "    (s_id, params) and results are pushed as (s_id, result).  The same\n",
    "    parameter can appear multiple times in p_queue, displaying the same id\n",
    "    each time.\n",
    "\n",
    "    As soon as e_queue is non empty, the thread terminate.\n",
    "\n",
    "    When multiple gpus are involved, the assigned gpu is determined by the\n",
    "    process index p_index (gpu = p_index % n_gpus).\n",
    "\n",
    "    :args p_queue: queue containing couples (s_id, parameters) to evaluate\n",
    "    :args r_queue: where to place results (s_id, results)\n",
    "    :args e_queue: as soon as not empty, terminate\n",
    "    :args p_index: the process index\n",
    "    \"\"\"\n",
    "    # init routine\n",
    "    # gpu = p_index % torch.cuda.device_count()\n",
    "    # device = torch.device('cuda:{}'.format(gpu) if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Check if CUDA-capable GPUs are available\n",
    "    if torch.cuda.is_available() and torch.cuda.device_count() > 0:\n",
    "        gpu = p_index % torch.cuda.device_count()\n",
    "        device = torch.device('cuda:{}'.format(gpu))\n",
    "    else:\n",
    "        # Use CPU if no GPUs are available\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    # redirect streams\n",
    "    sys.stdout = open(join(tmp_dir, str(getpid()) + '.out'), 'a')\n",
    "    sys.stderr = open(join(tmp_dir, str(getpid()) + '.err'), 'a') \n",
    "\n",
    "    with torch.no_grad():\n",
    "        r_gen = RolloutGenerator(args['logdir'], device, time_limit)\n",
    "        print(\"Rollout generator process {} ready!\".format(p_index))\n",
    "        while e_queue.empty():\n",
    "            if p_queue.empty():\n",
    "                sleep(.1)\n",
    "            else:\n",
    "                s_id, params = p_queue.get()\n",
    "                r_queue.put((s_id, r_gen.rollout(params)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                           Evaluation                                         #\n",
    "################################################################################\n",
    "def evaluate(solutions, results, rollouts=100):\n",
    "    \"\"\" Give current controller evaluation.\n",
    "\n",
    "    Evaluation is minus the cumulated reward averaged over rollout runs.\n",
    "\n",
    "    :args solutions: CMA set of solutions\n",
    "    :args results: corresponding results\n",
    "    :args rollouts: number of rollouts\n",
    "\n",
    "    :returns: minus averaged cumulated reward\n",
    "    \"\"\"\n",
    "    index_min = np.argmin(results)\n",
    "    best_guess = solutions[index_min]\n",
    "    restimates = []\n",
    "\n",
    "    for s_id in range(rollouts):\n",
    "        p_queue.put((s_id, best_guess))\n",
    "\n",
    "    print(\"Evaluating...\")\n",
    "    for _ in tqdm(range(rollouts)):\n",
    "        while r_queue.empty():\n",
    "            sleep(.1)\n",
    "        restimates.append(r_queue.get()[1])\n",
    "\n",
    "    print(\"Evaluation finished: \", np.mean(restimates))\n",
    "    return best_guess, np.mean(restimates), np.std(restimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load previous best...\n",
      "Previous best was -8107804.96...\n",
      "(2_w,4mirr1)-aCMA-ES (mu_w=1.5,w_1=80%) in dimension 760 (seed=1074881, Thu Jan 11 10:51:33 2024)\n",
      "\n",
      "********** Generation 0 ************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Making new env load_balance\n",
      "INFO:root:Making new env load_balance\n",
      "100%|██████████| 16/16 [00:04<00:00,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterat #Fevals   function value  axis ratio  sigma  min&max std  t[m:s]\n",
      "    1      4 7.986229000000000e+06 1.0e+00 9.94e-02  1e-01  1e-01 0:04.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** Generation 1 ************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:02<00:00,  7.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    2      8 9.486152000000000e+06 1.0e+00 9.88e-02  1e-01  1e-01 0:06.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** Generation 2 ************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:02<00:00,  7.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    3     12 9.722082500000000e+06 1.0e+00 9.82e-02  1e-01  1e-01 0:08.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  7.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation finished:  15323914.36\n",
      "Current evaluation: -15323914.36\n",
      "Current overall best: -8107804.96\n",
      "\n",
      "********** Generation 3 ************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:02<00:00,  7.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** Generation 4 ************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:02<00:00,  7.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    5     20 9.075379500000000e+06 1.0e+00 9.72e-02  1e-01  1e-01 0:13.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** Generation 5 ************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:02<00:00,  7.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:13<00:00,  7.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation finished:  12234831.92\n",
      "Current evaluation: -12234831.92\n",
      "Current overall best: -8107804.96\n",
      "\n",
      "********** Generation 6 ************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:02<00:00,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    7     28 5.117705500000000e+06 1.0e+00 9.62e-02  1e-01  1e-01 0:17.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** Generation 7 ************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(pop_size \u001b[38;5;241m*\u001b[39m n_samples):\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m r_queue\u001b[38;5;241m.\u001b[39mempty():\n\u001b[0;32m---> 55\u001b[0m         sleep(\u001b[38;5;241m.1\u001b[39m)\n\u001b[1;32m     56\u001b[0m     r_s_id, r \u001b[38;5;241m=\u001b[39m r_queue\u001b[38;5;241m.\u001b[39mget()\n\u001b[1;32m     57\u001b[0m     r_list[r_s_id] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m r \u001b[38;5;241m/\u001b[39m n_samples\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "#                           Launch CMA                                         #\n",
    "################################################################################\n",
    "if __name__ == '__main__':\n",
    "    # define queues and set workers\n",
    "    p_queue, r_queue, e_queue = set_slave_routines(num_workers, tmp_dir, args, time_limit)\n",
    "    \n",
    "    controller = Controller(LSIZE, RSIZE, ASIZE)  # dummy instance\n",
    "\n",
    "    # define current best and load parameters\n",
    "    cur_best = None\n",
    "    ctrl_file = join(ctrl_dir, 'best.tar')\n",
    "    print(\"Attempting to load previous best...\")\n",
    "    if exists(ctrl_file):\n",
    "        state = torch.load(ctrl_file, map_location={'cuda:0': 'cpu'})\n",
    "        cur_best = - state['reward']\n",
    "        controller.load_state_dict(state['state_dict'])\n",
    "        print(\"Previous best was {}...\".format(-cur_best))\n",
    "\n",
    "    parameters = controller.parameters()\n",
    "    es = cma.CMAEvolutionStrategy(flatten_parameters(parameters), 0.1,\n",
    "                                {'popsize': pop_size})\n",
    "\n",
    "    epoch = 0\n",
    "    log_step = 3\n",
    "\n",
    "    # p_queue, r_queue, e_queue = set_queues(num_workers)\n",
    "\n",
    "    while not es.stop():\n",
    "        print(\"\\n********** Generation {} ************\".format(epoch))\n",
    "        if cur_best is not None and - cur_best > args['target_return']:\n",
    "            print(\"Already better than target, breaking...\")\n",
    "            break\n",
    "\n",
    "        r_list = [0] * pop_size  # result list\n",
    "        solutions = es.ask()\n",
    "\n",
    "        # push parameters to queue\n",
    "        for s_id, s in enumerate(solutions):\n",
    "            for _ in range(n_samples):\n",
    "                p_queue.put((s_id, s))\n",
    "\n",
    "        # retrieve results\n",
    "        if args['display']:\n",
    "            pbar = tqdm(total=pop_size * n_samples)\n",
    "        for _ in range(pop_size * n_samples):\n",
    "            while r_queue.empty():\n",
    "                sleep(.1)\n",
    "            r_s_id, r = r_queue.get()\n",
    "            r_list[r_s_id] += r / n_samples\n",
    "            if args['display']:\n",
    "                pbar.update(1)\n",
    "        if args['display']:\n",
    "            pbar.close()\n",
    "\n",
    "        es.tell(solutions, r_list)\n",
    "        es.disp()\n",
    "\n",
    "        # evaluation and saving\n",
    "        if epoch % log_step == log_step - 1:\n",
    "            best_params, best, std_best = evaluate(solutions, r_list)\n",
    "            print(\"Current evaluation: {}\".format(-best))\n",
    "            if cur_best != None: print(\"Current overall best: {}\".format(-cur_best))\n",
    "            if not cur_best or best < cur_best:\n",
    "                cur_best = best\n",
    "                print(\"Saving new best with value {}+-{}...\".format(-cur_best, std_best))\n",
    "                load_parameters(best_params, controller)\n",
    "                torch.save(\n",
    "                    {'epoch': epoch,\n",
    "                    'reward': - cur_best,\n",
    "                    'state_dict': controller.state_dict()},\n",
    "                    join(ctrl_dir, 'best.tar'))\n",
    "            if - best > args['target_return']:\n",
    "                print(\"Terminating controller training with value {}...\".format(-best))\n",
    "                break\n",
    "\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "    es.result_pretty()\n",
    "    e_queue.put('EOP')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
