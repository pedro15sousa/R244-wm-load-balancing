{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training a linear controller on latent + recurrent state\n",
    "with CMAES.\n",
    "\n",
    "This is a bit complex. num_workers slave threads are launched\n",
    "to process a queue filled with parameters to be evaluated.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import sys\n",
    "from os.path import join, exists\n",
    "from os import mkdir, unlink, listdir, getpid\n",
    "from time import sleep\n",
    "from torch.multiprocessing import Process, Queue\n",
    "import torch\n",
    "import cma\n",
    "from models import Controller\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from utils.misc import RolloutGenerator, ASIZE, RSIZE, LSIZE\n",
    "from utils.misc import load_parameters\n",
    "from utils.misc import flatten_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--logdir', type=str, help='Where everything is stored.')\n",
    "parser.add_argument('--n-samples', type=int, help='Number of samples used to obtain '\n",
    "                    'return estimate.')\n",
    "parser.add_argument('--pop-size', type=int, help='Population size.')\n",
    "parser.add_argument('--target-return', type=float, help='Stops once the return '\n",
    "                    'gets above target_return')\n",
    "parser.add_argument('--display', action='store_true', help=\"Use progress bars if \"\n",
    "                    \"specified.\")\n",
    "parser.add_argument('--max-workers', type=int, help='Maximum number of workers.',\n",
    "                    default=32)\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max number of workers. M\n",
    "\n",
    "# multiprocessing variables\n",
    "n_samples = args.n_samples\n",
    "pop_size = args.pop_size\n",
    "num_workers = min(args.max_workers, n_samples * pop_size)\n",
    "time_limit = 1000\n",
    "\n",
    "# create tmp dir if non existent and clean it if existent\n",
    "tmp_dir = join(args.logdir, 'tmp')\n",
    "if not exists(tmp_dir):\n",
    "    mkdir(tmp_dir)\n",
    "else:\n",
    "    for fname in listdir(tmp_dir):\n",
    "        unlink(join(tmp_dir, fname))\n",
    "\n",
    "# create ctrl dir if non exitent\n",
    "ctrl_dir = join(args.logdir, 'ctrl')\n",
    "if not exists(ctrl_dir):\n",
    "    mkdir(ctrl_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                           Thread routines                                    #\n",
    "################################################################################\n",
    "def slave_routine(p_queue, r_queue, e_queue, p_index):\n",
    "    \"\"\" Thread routine.\n",
    "\n",
    "    Threads interact with p_queue, the parameters queue, r_queue, the result\n",
    "    queue and e_queue the end queue. They pull parameters from p_queue, execute\n",
    "    the corresponding rollout, then place the result in r_queue.\n",
    "\n",
    "    Each parameter has its own unique id. Parameters are pulled as tuples\n",
    "    (s_id, params) and results are pushed as (s_id, result).  The same\n",
    "    parameter can appear multiple times in p_queue, displaying the same id\n",
    "    each time.\n",
    "\n",
    "    As soon as e_queue is non empty, the thread terminate.\n",
    "\n",
    "    When multiple gpus are involved, the assigned gpu is determined by the\n",
    "    process index p_index (gpu = p_index % n_gpus).\n",
    "\n",
    "    :args p_queue: queue containing couples (s_id, parameters) to evaluate\n",
    "    :args r_queue: where to place results (s_id, results)\n",
    "    :args e_queue: as soon as not empty, terminate\n",
    "    :args p_index: the process index\n",
    "    \"\"\"\n",
    "    # init routine\n",
    "    # gpu = p_index % torch.cuda.device_count()\n",
    "    # device = torch.device('cuda:{}'.format(gpu) if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Check if CUDA-capable GPUs are available\n",
    "    if torch.cuda.is_available() and torch.cuda.device_count() > 0:\n",
    "        gpu = p_index % torch.cuda.device_count()\n",
    "        device = torch.device('cuda:{}'.format(gpu))\n",
    "    else:\n",
    "        # Use CPU if no GPUs are available\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    # redirect streams\n",
    "    sys.stdout = open(join(tmp_dir, str(getpid()) + '.out'), 'a')\n",
    "    sys.stderr = open(join(tmp_dir, str(getpid()) + '.err'), 'a') \n",
    "\n",
    "    with torch.no_grad():\n",
    "        r_gen = RolloutGenerator(args.logdir, device, time_limit)\n",
    "        print(\"Rollout generator process {} ready!\".format(p_index))\n",
    "        while e_queue.empty():\n",
    "            if p_queue.empty():\n",
    "                sleep(.1)\n",
    "            else:\n",
    "                s_id, params = p_queue.get()\n",
    "                r_queue.put((s_id, r_gen.rollout(params)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                           Evaluation                                         #\n",
    "################################################################################\n",
    "def evaluate(solutions, results, rollouts=100):\n",
    "    \"\"\" Give current controller evaluation.\n",
    "\n",
    "    Evaluation is minus the cumulated reward averaged over rollout runs.\n",
    "\n",
    "    :args solutions: CMA set of solutions\n",
    "    :args results: corresponding results\n",
    "    :args rollouts: number of rollouts\n",
    "\n",
    "    :returns: minus averaged cumulated reward\n",
    "    \"\"\"\n",
    "    index_min = np.argmin(results)\n",
    "    best_guess = solutions[index_min]\n",
    "    restimates = []\n",
    "\n",
    "    for s_id in range(rollouts):\n",
    "        p_queue.put((s_id, best_guess))\n",
    "\n",
    "    print(\"Evaluating...\")\n",
    "    for _ in tqdm(range(rollouts)):\n",
    "        while r_queue.empty():\n",
    "            sleep(.1)\n",
    "        restimates.append(r_queue.get()[1])\n",
    "\n",
    "    return best_guess, np.mean(restimates), np.std(restimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                           Launch CMA                                         #\n",
    "################################################################################\n",
    "if __name__ == '__main__':\n",
    "    # define queues and set workers\n",
    "    p_queue = Queue()\n",
    "    r_queue = Queue()\n",
    "    e_queue = Queue()\n",
    "\n",
    "    for p_index in range(num_workers):\n",
    "        Process(target=slave_routine, args=(p_queue, r_queue, e_queue, p_index)).start()\n",
    "    \n",
    "    controller = Controller(LSIZE, RSIZE, ASIZE)  # dummy instance\n",
    "\n",
    "    # define current best and load parameters\n",
    "    cur_best = None\n",
    "    ctrl_file = join(ctrl_dir, 'best.tar')\n",
    "    print(\"Attempting to load previous best...\")\n",
    "    if exists(ctrl_file):\n",
    "        state = torch.load(ctrl_file, map_location={'cuda:0': 'cpu'})\n",
    "        cur_best = - state['reward']\n",
    "        controller.load_state_dict(state['state_dict'])\n",
    "        print(\"Previous best was {}...\".format(-cur_best))\n",
    "\n",
    "    parameters = controller.parameters()\n",
    "    es = cma.CMAEvolutionStrategy(flatten_parameters(parameters), 0.1,\n",
    "                                {'popsize': pop_size})\n",
    "\n",
    "    epoch = 0\n",
    "    log_step = 3\n",
    "\n",
    "    # p_queue, r_queue, e_queue = set_queues(num_workers)\n",
    "\n",
    "    while not es.stop():\n",
    "        if cur_best is not None and - cur_best > args.target_return:\n",
    "            print(\"Already better than target, breaking...\")\n",
    "            break\n",
    "\n",
    "        r_list = [0] * pop_size  # result list\n",
    "        solutions = es.ask()\n",
    "\n",
    "        # push parameters to queue\n",
    "        for s_id, s in enumerate(solutions):\n",
    "            for _ in range(n_samples):\n",
    "                p_queue.put((s_id, s))\n",
    "\n",
    "        # retrieve results\n",
    "        if args.display:\n",
    "            pbar = tqdm(total=pop_size * n_samples)\n",
    "        for _ in range(pop_size * n_samples):\n",
    "            while r_queue.empty():\n",
    "                sleep(.1)\n",
    "            r_s_id, r = r_queue.get()\n",
    "            r_list[r_s_id] += r / n_samples\n",
    "            if args.display:\n",
    "                pbar.update(1)\n",
    "        if args.display:\n",
    "            pbar.close()\n",
    "\n",
    "        es.tell(solutions, r_list)\n",
    "        es.disp()\n",
    "\n",
    "        # evaluation and saving\n",
    "        if epoch % log_step == log_step - 1:\n",
    "            best_params, best, std_best = evaluate(solutions, r_list)\n",
    "            print(\"Current evaluation: {}\".format(-best))\n",
    "            if not cur_best or cur_best < best:\n",
    "                cur_best = best\n",
    "                print(\"Saving new best with value {}+-{}...\".format(-cur_best, std_best))\n",
    "                load_parameters(best_params, controller)\n",
    "                torch.save(\n",
    "                    {'epoch': epoch,\n",
    "                    'reward': - cur_best,\n",
    "                    'state_dict': controller.state_dict()},\n",
    "                    join(ctrl_dir, 'best.tar'))\n",
    "            if - best > args.target_return:\n",
    "                print(\"Terminating controller training with value {}...\".format(-best))\n",
    "                break\n",
    "\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "    es.result_pretty()\n",
    "    e_queue.put('EOP')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
