{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in /Users/pedrosousa/anaconda3/lib/python3.11/site-packages (2.15.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/pedrosousa/anaconda3/lib/python3.11/site-packages (from tensorboard) (2.0.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/pedrosousa/anaconda3/lib/python3.11/site-packages (from tensorboard) (1.60.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/pedrosousa/anaconda3/lib/python3.11/site-packages (from tensorboard) (2.26.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /Users/pedrosousa/anaconda3/lib/python3.11/site-packages (from tensorboard) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/pedrosousa/anaconda3/lib/python3.11/site-packages (from tensorboard) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /Users/pedrosousa/anaconda3/lib/python3.11/site-packages (from tensorboard) (1.24.3)\n",
      "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /Users/pedrosousa/anaconda3/lib/python3.11/site-packages (from tensorboard) (4.23.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/pedrosousa/anaconda3/lib/python3.11/site-packages (from tensorboard) (2.31.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/pedrosousa/anaconda3/lib/python3.11/site-packages (from tensorboard) (69.0.3)\n",
      "Requirement already satisfied: six>1.9 in /Users/pedrosousa/anaconda3/lib/python3.11/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/pedrosousa/anaconda3/lib/python3.11/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/pedrosousa/anaconda3/lib/python3.11/site-packages (from tensorboard) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/pedrosousa/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/pedrosousa/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/pedrosousa/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/pedrosousa/anaconda3/lib/python3.11/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/pedrosousa/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pedrosousa/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/pedrosousa/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pedrosousa/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/pedrosousa/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/pedrosousa/anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/pedrosousa/anaconda3/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on Google Colab. Assuming local environment.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check if the notebook is running on Colab\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    # This block will run only in Google Colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running on Google Colab. Cloning the repository.\")\n",
    "    !git clone https://github.com/pedro15sousa/R244-wm-load-balancing.git\n",
    "    %cd R244-wm-load-balancing/notebooks\n",
    "else: \n",
    "    # This block will run if not in Google Colab\n",
    "    IN_COLAB = False\n",
    "    print(\"Not running on Google Colab. Assuming local environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')  # This adds the parent directory (main_folder) to the Python path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from os.path import join, exists\n",
    "from os import mkdir\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from models.vae import VAE\n",
    "\n",
    "from utils.misc import save_checkpoint\n",
    "from utils.misc import LSIZE, RED_SIZE, ASIZE\n",
    "## WARNING : THIS SHOULD BE REPLACE WITH PYTORCH 0.5\n",
    "from utils.learning import EarlyStopping\n",
    "from utils.learning import ReduceLROnPlateau\n",
    "from data.loaders import RolloutObservationDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description='VAE Trainer')\n",
    "# parser.add_argument('--batch-size', type=int, default=32, metavar='N',\n",
    "#                     help='input batch size for training (default: 32)')\n",
    "# parser.add_argument('--epochs', type=int, default=1000, metavar='N',\n",
    "#                     help='number of epochs to train (default: 1000)')\n",
    "# parser.add_argument('--logdir', type=str, help='Directory where results are logged')\n",
    "# parser.add_argument('--noreload', action='store_true',\n",
    "#                     help='Best model is not reloaded if specified')\n",
    "# parser.add_argument('--nosamples', action='store_true',\n",
    "#                     help='Does not save samples during training if specified')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "args = {\n",
    "    'batch_size': 32,   # input batch size for training (default: 32)\n",
    "    'epochs': 1,     # number of epochs to train (default: 1000)\n",
    "    'logdir': '../exp_dir',  # Directory where results are logged\n",
    "    'noreload': False,  # Set True if best model is not to be reloaded\n",
    "    'nosamples': False,  # Set True if samples are not to be saved during training\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "# Fix numeric divergence due to bug in Cudnn\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "# 51 is the number of dimensions in my space: 50 servers + 1 job size\n",
    "model = VAE(ASIZE+1, LSIZE).to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5)\n",
    "earlystopping = EarlyStopping('min', patience=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    dataset_train = RolloutObservationDataset('../datasets/loadbalancing', train=True)\n",
    "    dataset_test = RolloutObservationDataset('../datasets/loadbalancing',train=False)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset_train, batch_size=args['batch_size'], shuffle=True, num_workers=2)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=args['batch_size'], shuffle=True, num_workers=2)\n",
    "\n",
    "    return dataset_test, dataset_train, test_loader, train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logsigma):\n",
    "    \"\"\" VAE loss function \"\"\"\n",
    "    BCE = F.mse_loss(recon_x, x, size_average=False)\n",
    "    # print(mu)\n",
    "    # print(logsigma)\n",
    "    # print(recon_x)\n",
    "    # print(x)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + 2 * logsigma - mu.pow(2) - (2 * logsigma).exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    \"\"\" One training epoch \"\"\"\n",
    "    model.train()\n",
    "    dataset_train.load_next_buffer()\n",
    "    train_loss = 0\n",
    "    print(\"\\nTrain Loader size: \", len(train_loader))\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = data.float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 20 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "        epoch, train_loss / len(train_loader.dataset)))\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    \"\"\" One test epoch \"\"\"\n",
    "    model.eval()\n",
    "    dataset_test.load_next_buffer()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.float().to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vae_single_sample(model, test_loader, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get a single batch from the test loader\n",
    "        data = next(iter(test_loader))[0].to(device)\n",
    "        \n",
    "        # Select the first sample from the batch\n",
    "        sample = data.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        print(sample)\n",
    "        print(\"Sample shape:\", sample.shape)  # Add this line to check the shape\n",
    "\n",
    "        # Forward pass through the VAE\n",
    "        recon, mu, logsigma = model(sample)\n",
    "\n",
    "        sigma = logsigma.exp()\n",
    "        eps = torch.randn_like(sigma)\n",
    "        z = eps.mul(sigma).add_(mu)\n",
    "\n",
    "        # Convert tensors to numpy arrays for visualization\n",
    "        sample_np = sample.squeeze().cpu().numpy()\n",
    "        recon_np = recon.squeeze().cpu().numpy()\n",
    "        z_np = z.squeeze().cpu().numpy()\n",
    "        \n",
    "        print(\"\\nOriginal Sample: \", sample_np)\n",
    "        print(\"Reconstructed Sample: \", recon_np)\n",
    "        print(\"Latent Space Sample: \", z_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    %reload_ext tensorboard\n",
    "    %tensorboard --logdir ../exp_dir/MNIST/vae/tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading file buffer ...: 100%|██████████| 1000/1000 \n",
      "Loading file buffer ...: 100%|██████████| 1000/1000 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading model at epoch 1, with test error 0.7008767317237854\n",
      "tensor([[1.0000, 0.3363, 0.2604, 0.0525, 0.3730, 0.0000, 0.0787, 0.1480, 0.0000,\n",
      "         0.0000, 0.0513]])\n",
      "Sample shape: torch.Size([1, 11])\n",
      "\n",
      "Original Sample:  [1.         0.3362939  0.26039565 0.05248284 0.37303188 0.\n",
      " 0.07872427 0.14796124 0.         0.         0.0512717 ]\n",
      "Reconstructed Sample:  [0.70486295 0.52887386 0.35667124 0.26748252 0.17934649 0.12874219\n",
      " 0.10001577 0.07928371 0.07021698 0.06133177 0.03459665]\n",
      "Latent Space Sample:  [ 0.35530996 -0.03078336  0.60374904  1.103252   -0.701139   -0.02517617\n",
      "  0.2703277   1.7393682   0.07784095 -0.27177042 -1.8276705 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading file buffer ...: 100%|██████████| 1000/1000 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loader size:  31250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/1000000 (0%)]\tLoss: 0.641733\n",
      "Train Epoch: 1 [640/1000000 (0%)]\tLoss: 0.660052\n",
      "Train Epoch: 1 [1280/1000000 (0%)]\tLoss: 0.693514\n",
      "Train Epoch: 1 [1920/1000000 (0%)]\tLoss: 0.659740\n",
      "Train Epoch: 1 [2560/1000000 (0%)]\tLoss: 0.613493\n",
      "Train Epoch: 1 [3200/1000000 (0%)]\tLoss: 0.628386\n",
      "Train Epoch: 1 [3840/1000000 (0%)]\tLoss: 0.621188\n",
      "Train Epoch: 1 [4480/1000000 (0%)]\tLoss: 0.672482\n",
      "Train Epoch: 1 [5120/1000000 (1%)]\tLoss: 0.684296\n",
      "Train Epoch: 1 [5760/1000000 (1%)]\tLoss: 0.639472\n",
      "Train Epoch: 1 [6400/1000000 (1%)]\tLoss: 0.716257\n",
      "Train Epoch: 1 [7040/1000000 (1%)]\tLoss: 0.620353\n",
      "Train Epoch: 1 [7680/1000000 (1%)]\tLoss: 0.724332\n",
      "Train Epoch: 1 [8320/1000000 (1%)]\tLoss: 0.706579\n",
      "Train Epoch: 1 [8960/1000000 (1%)]\tLoss: 0.678379\n",
      "Train Epoch: 1 [9600/1000000 (1%)]\tLoss: 0.714814\n",
      "Train Epoch: 1 [10240/1000000 (1%)]\tLoss: 0.619089\n",
      "Train Epoch: 1 [10880/1000000 (1%)]\tLoss: 0.740326\n",
      "Train Epoch: 1 [11520/1000000 (1%)]\tLoss: 0.809430\n",
      "Train Epoch: 1 [12160/1000000 (1%)]\tLoss: 0.604790\n",
      "Train Epoch: 1 [12800/1000000 (1%)]\tLoss: 0.676665\n",
      "Train Epoch: 1 [13440/1000000 (1%)]\tLoss: 0.737457\n",
      "Train Epoch: 1 [14080/1000000 (1%)]\tLoss: 0.702608\n",
      "Train Epoch: 1 [14720/1000000 (1%)]\tLoss: 0.663791\n",
      "Train Epoch: 1 [15360/1000000 (2%)]\tLoss: 0.699864\n",
      "Train Epoch: 1 [16000/1000000 (2%)]\tLoss: 0.746657\n",
      "Train Epoch: 1 [16640/1000000 (2%)]\tLoss: 0.760634\n",
      "Train Epoch: 1 [17280/1000000 (2%)]\tLoss: 0.751886\n",
      "Train Epoch: 1 [17920/1000000 (2%)]\tLoss: 0.818733\n",
      "Train Epoch: 1 [18560/1000000 (2%)]\tLoss: 0.645648\n",
      "Train Epoch: 1 [19200/1000000 (2%)]\tLoss: 0.684934\n",
      "Train Epoch: 1 [19840/1000000 (2%)]\tLoss: 0.743706\n",
      "Train Epoch: 1 [20480/1000000 (2%)]\tLoss: 0.682112\n",
      "Train Epoch: 1 [21120/1000000 (2%)]\tLoss: 0.643325\n",
      "Train Epoch: 1 [21760/1000000 (2%)]\tLoss: 0.804254\n",
      "Train Epoch: 1 [22400/1000000 (2%)]\tLoss: 0.698527\n",
      "Train Epoch: 1 [23040/1000000 (2%)]\tLoss: 0.689644\n",
      "Train Epoch: 1 [23680/1000000 (2%)]\tLoss: 0.636664\n",
      "Train Epoch: 1 [24320/1000000 (2%)]\tLoss: 0.685424\n",
      "Train Epoch: 1 [24960/1000000 (2%)]\tLoss: 0.772688\n",
      "Train Epoch: 1 [25600/1000000 (3%)]\tLoss: 0.597231\n",
      "Train Epoch: 1 [26240/1000000 (3%)]\tLoss: 0.611195\n",
      "Train Epoch: 1 [26880/1000000 (3%)]\tLoss: 0.605285\n",
      "Train Epoch: 1 [27520/1000000 (3%)]\tLoss: 0.629579\n",
      "Train Epoch: 1 [28160/1000000 (3%)]\tLoss: 0.750269\n",
      "Train Epoch: 1 [28800/1000000 (3%)]\tLoss: 0.728008\n",
      "Train Epoch: 1 [29440/1000000 (3%)]\tLoss: 0.661034\n",
      "Train Epoch: 1 [30080/1000000 (3%)]\tLoss: 0.681722\n",
      "Train Epoch: 1 [30720/1000000 (3%)]\tLoss: 0.693685\n",
      "Train Epoch: 1 [31360/1000000 (3%)]\tLoss: 0.704128\n",
      "Train Epoch: 1 [32000/1000000 (3%)]\tLoss: 0.704853\n",
      "Train Epoch: 1 [32640/1000000 (3%)]\tLoss: 0.780117\n",
      "Train Epoch: 1 [33280/1000000 (3%)]\tLoss: 0.700499\n",
      "Train Epoch: 1 [33920/1000000 (3%)]\tLoss: 0.713988\n",
      "Train Epoch: 1 [34560/1000000 (3%)]\tLoss: 0.688058\n",
      "Train Epoch: 1 [35200/1000000 (4%)]\tLoss: 0.735498\n",
      "Train Epoch: 1 [35840/1000000 (4%)]\tLoss: 0.691763\n",
      "Train Epoch: 1 [36480/1000000 (4%)]\tLoss: 0.702765\n",
      "Train Epoch: 1 [37120/1000000 (4%)]\tLoss: 0.653857\n",
      "Train Epoch: 1 [37760/1000000 (4%)]\tLoss: 0.687020\n",
      "Train Epoch: 1 [38400/1000000 (4%)]\tLoss: 0.801916\n",
      "Train Epoch: 1 [39040/1000000 (4%)]\tLoss: 0.667023\n",
      "Train Epoch: 1 [39680/1000000 (4%)]\tLoss: 0.672150\n",
      "Train Epoch: 1 [40320/1000000 (4%)]\tLoss: 0.618433\n",
      "Train Epoch: 1 [40960/1000000 (4%)]\tLoss: 0.617965\n",
      "Train Epoch: 1 [41600/1000000 (4%)]\tLoss: 0.644711\n",
      "Train Epoch: 1 [42240/1000000 (4%)]\tLoss: 0.659603\n",
      "Train Epoch: 1 [42880/1000000 (4%)]\tLoss: 0.778155\n",
      "Train Epoch: 1 [43520/1000000 (4%)]\tLoss: 0.629393\n",
      "Train Epoch: 1 [44160/1000000 (4%)]\tLoss: 0.745547\n",
      "Train Epoch: 1 [44800/1000000 (4%)]\tLoss: 0.792647\n",
      "Train Epoch: 1 [45440/1000000 (5%)]\tLoss: 0.748082\n",
      "Train Epoch: 1 [46080/1000000 (5%)]\tLoss: 0.644881\n",
      "Train Epoch: 1 [46720/1000000 (5%)]\tLoss: 0.675850\n",
      "Train Epoch: 1 [47360/1000000 (5%)]\tLoss: 0.808426\n",
      "Train Epoch: 1 [48000/1000000 (5%)]\tLoss: 0.735225\n",
      "Train Epoch: 1 [48640/1000000 (5%)]\tLoss: 0.675044\n",
      "Train Epoch: 1 [49280/1000000 (5%)]\tLoss: 0.653130\n",
      "Train Epoch: 1 [49920/1000000 (5%)]\tLoss: 0.739839\n",
      "Train Epoch: 1 [50560/1000000 (5%)]\tLoss: 0.606139\n",
      "Train Epoch: 1 [51200/1000000 (5%)]\tLoss: 0.572947\n",
      "Train Epoch: 1 [51840/1000000 (5%)]\tLoss: 0.788325\n",
      "Train Epoch: 1 [52480/1000000 (5%)]\tLoss: 0.609108\n",
      "Train Epoch: 1 [53120/1000000 (5%)]\tLoss: 0.656253\n",
      "Train Epoch: 1 [53760/1000000 (5%)]\tLoss: 0.619352\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m cur_best \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 34\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train(epoch)\n\u001b[1;32m     35\u001b[0m     test_loss \u001b[38;5;241m=\u001b[39m test()\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# TensorBoard logging\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     12\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     13\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Epoch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m [\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{:.0f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m)]\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{:.6f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     17\u001b[0m         epoch, batch_idx \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(data), \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset),\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;241m100.\u001b[39m \u001b[38;5;241m*\u001b[39m batch_idx \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader),\n\u001b[1;32m     19\u001b[0m         loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(data)))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     adam(\n\u001b[1;32m    164\u001b[0m         params_with_grad,\n\u001b[1;32m    165\u001b[0m         grads,\n\u001b[1;32m    166\u001b[0m         exp_avgs,\n\u001b[1;32m    167\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    168\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    169\u001b[0m         state_steps,\n\u001b[1;32m    170\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    171\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    172\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    173\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    174\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    175\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    176\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    177\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    178\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    179\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    180\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    181\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    182\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m func(params,\n\u001b[1;32m    312\u001b[0m      grads,\n\u001b[1;32m    313\u001b[0m      exp_avgs,\n\u001b[1;32m    314\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    315\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    316\u001b[0m      state_steps,\n\u001b[1;32m    317\u001b[0m      amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    318\u001b[0m      beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    319\u001b[0m      beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    320\u001b[0m      lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m    321\u001b[0m      weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[1;32m    322\u001b[0m      eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    323\u001b[0m      maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[1;32m    324\u001b[0m      capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m    325\u001b[0m      differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[1;32m    326\u001b[0m      grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[1;32m    327\u001b[0m      found_inf\u001b[38;5;241m=\u001b[39mfound_inf)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:384\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    381\u001b[0m     param \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(param)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 384\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[1;32m    385\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize the TensorBoard writer\n",
    "    logdir = args['logdir']\n",
    "    writer = SummaryWriter(os.path.join(logdir, 'vae/tensorboard'))\n",
    "\n",
    "    # load data\n",
    "    dataset_test, dataset_train, test_loader, train_loader = load_data()\n",
    "\n",
    "    # check vae dir exists, if not, create it\n",
    "    vae_dir = join(args['logdir'], 'vae')\n",
    "    if not exists(vae_dir):\n",
    "        mkdir(vae_dir)\n",
    "        mkdir(join(vae_dir, 'samples'))\n",
    "\n",
    "    reload_file = join(vae_dir, 'best.tar')\n",
    "    if not args['noreload'] and exists(reload_file):\n",
    "        state = torch.load(reload_file)\n",
    "        print(\"Reloading model at epoch {}\"\n",
    "            \", with test error {}\".format(\n",
    "                state['epoch'],\n",
    "                state['precision']))\n",
    "        model.load_state_dict(state['state_dict'])\n",
    "        optimizer.load_state_dict(state['optimizer'])\n",
    "        scheduler.load_state_dict(state['scheduler'])\n",
    "        earlystopping.load_state_dict(state['earlystopping'])\n",
    "\n",
    "        # Assuming you have a trained VAE model, a test_loader, and a device (e.g., 'cuda' or 'cpu')\n",
    "        test_vae_single_sample(model, test_loader, device)\n",
    "\n",
    "\n",
    "    cur_best = None\n",
    "\n",
    "    for epoch in range(1, args['epochs'] + 1):\n",
    "        train_loss = train(epoch)\n",
    "        test_loss = test()\n",
    "\n",
    "        # TensorBoard logging\n",
    "        writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "        writer.add_scalar('Loss/test', test_loss, epoch)\n",
    "\n",
    "        scheduler.step(test_loss)\n",
    "        earlystopping.step(test_loss)\n",
    "\n",
    "        # checkpointing\n",
    "        best_filename = join(vae_dir, 'best.tar')\n",
    "        filename = join(vae_dir, 'checkpoint.tar')\n",
    "        is_best = not cur_best or test_loss < cur_best\n",
    "        if is_best:\n",
    "            cur_best = test_loss\n",
    "\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'precision': test_loss,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'earlystopping': earlystopping.state_dict()\n",
    "        }, is_best, filename, best_filename)\n",
    "\n",
    "        # if not args['nosamples']:\n",
    "        #     with torch.no_grad():\n",
    "        #         sample = torch.randn(RED_SIZE, LSIZE).to(device)\n",
    "        #         sample = model.decoder(sample).cpu()\n",
    "        #         save_image(sample.view(64, 3, RED_SIZE, RED_SIZE),\n",
    "        #                 join(vae_dir, 'samples/sample_' + str(epoch) + '.png'))\n",
    "\n",
    "        if earlystopping.stop:\n",
    "            print(\"End of Training because of early stopping at epoch {}\".format(epoch))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
