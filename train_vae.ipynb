{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from os.path import join, exists\n",
    "from os import mkdir\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from models.vae import VAE\n",
    "\n",
    "from utils.misc import save_checkpoint\n",
    "from utils.misc import LSIZE, RED_SIZE, ASIZE\n",
    "## WARNING : THIS SHOULD BE REPLACE WITH PYTORCH 0.5\n",
    "from utils.learning import EarlyStopping\n",
    "from utils.learning import ReduceLROnPlateau\n",
    "from data.loaders import RolloutObservationDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='VAE Trainer')\n",
    "parser.add_argument('--batch-size', type=int, default=32, metavar='N',\n",
    "                    help='input batch size for training (default: 32)')\n",
    "parser.add_argument('--epochs', type=int, default=1000, metavar='N',\n",
    "                    help='number of epochs to train (default: 1000)')\n",
    "parser.add_argument('--logdir', type=str, help='Directory where results are logged')\n",
    "parser.add_argument('--noreload', action='store_true',\n",
    "                    help='Best model is not reloaded if specified')\n",
    "parser.add_argument('--nosamples', action='store_true',\n",
    "                    help='Does not save samples during training if specified')\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "# Fix numeric divergence due to bug in Cudnn\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "# 51 is the number of dimensions in my space: 50 servers + 1 job size\n",
    "model = VAE(ASIZE+1, LSIZE).to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5)\n",
    "earlystopping = EarlyStopping('min', patience=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    dataset_train = RolloutObservationDataset('datasets/loadbalancing', train=True)\n",
    "    dataset_test = RolloutObservationDataset('datasets/loadbalancing',train=False)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset_train, batch_size=args.batch_size, shuffle=True, num_workers=2)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=args.batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    return dataset_test, dataset_train, test_loader, train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logsigma):\n",
    "    \"\"\" VAE loss function \"\"\"\n",
    "    BCE = F.mse_loss(recon_x, x, size_average=False)\n",
    "    # print(mu)\n",
    "    # print(logsigma)\n",
    "    # print(recon_x)\n",
    "    # print(x)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + 2 * logsigma - mu.pow(2) - (2 * logsigma).exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    \"\"\" One training epoch \"\"\"\n",
    "    model.train()\n",
    "    dataset_train.load_next_buffer()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = data.float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 20 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "        epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    \"\"\" One test epoch \"\"\"\n",
    "    model.eval()\n",
    "    dataset_test.load_next_buffer()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.float().to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # load data\n",
    "    dataset_test, dataset_train, test_loader, train_loader = load_data()\n",
    "\n",
    "    # check vae dir exists, if not, create it\n",
    "    vae_dir = join(args.logdir, 'vae')\n",
    "    if not exists(vae_dir):\n",
    "        mkdir(vae_dir)\n",
    "        mkdir(join(vae_dir, 'samples'))\n",
    "\n",
    "    reload_file = join(vae_dir, 'best.tar')\n",
    "    if not args.noreload and exists(reload_file):\n",
    "        state = torch.load(reload_file)\n",
    "        print(\"Reloading model at epoch {}\"\n",
    "            \", with test error {}\".format(\n",
    "                state['epoch'],\n",
    "                state['precision']))\n",
    "        model.load_state_dict(state['state_dict'])\n",
    "        optimizer.load_state_dict(state['optimizer'])\n",
    "        scheduler.load_state_dict(state['scheduler'])\n",
    "        earlystopping.load_state_dict(state['earlystopping'])\n",
    "\n",
    "\n",
    "    cur_best = None\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(epoch)\n",
    "        test_loss = test()\n",
    "        scheduler.step(test_loss)\n",
    "        earlystopping.step(test_loss)\n",
    "\n",
    "        # checkpointing\n",
    "        best_filename = join(vae_dir, 'best.tar')\n",
    "        filename = join(vae_dir, 'checkpoint.tar')\n",
    "        is_best = not cur_best or test_loss < cur_best\n",
    "        if is_best:\n",
    "            cur_best = test_loss\n",
    "\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'precision': test_loss,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'earlystopping': earlystopping.state_dict()\n",
    "        }, is_best, filename, best_filename)\n",
    "\n",
    "        if not args.nosamples:\n",
    "            with torch.no_grad():\n",
    "                sample = torch.randn(RED_SIZE, LSIZE).to(device)\n",
    "                sample = model.decoder(sample).cpu()\n",
    "                save_image(sample.view(64, 3, RED_SIZE, RED_SIZE),\n",
    "                        join(vae_dir, 'samples/sample_' + str(epoch) + '.png'))\n",
    "\n",
    "        if earlystopping.stop:\n",
    "            print(\"End of Training because of early stopping at epoch {}\".format(epoch))\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
